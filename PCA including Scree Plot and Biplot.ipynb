{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd24fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f977ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:\\FOLDER FROM THESIS\\THESIS\\Processed data\\Training ML\\filled_manipulated_28_11.geojson'\n",
    "\n",
    "# Load the .geojson file into a GeoDataFrame\n",
    "gdf = gpd.read_file(file_path)\n",
    "\n",
    "# Temporarily adjust display settings to show more columns\n",
    "with pd.option_context('display.max_columns', None, 'display.max_rows', None):\n",
    "    # Print the first 20 rows\n",
    "    print(\"First 20 rows:\")\n",
    "    print(gdf.head(20))\n",
    "\n",
    "    # Print the last 20 rows\n",
    "    print(\"\\nLast 20 rows:\")\n",
    "    print(gdf.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant fields\n",
    "columns_to_drop = ['byg021BygningensAnvendelse', 'geometry', 'byg404Koordinat', 'byg406Koordinatsystem', 'x', 'y']\n",
    "gdf = gdf.drop(columns=columns_to_drop)\n",
    "\n",
    "# One-Hot Encode Categorical Variables\n",
    "categorical_columns = ['byg032YdervæggensMateriale', 'byg033Tagdækningsmateriale', 'byg056Varmeinstallation', \n",
    "                       'eta006BygningensEtagebetegnelse', 'landscape', 'TSYM', 'byg021BygningensAnvendelse_grouped']\n",
    "gdf_encoded = pd.get_dummies(gdf, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns\n",
    "with pd.option_context('display.max_columns', None, 'display.max_rows', None):\n",
    "    # Print the first 20 rows\n",
    "    print(\"First 20 rows:\")\n",
    "    print(gdf_encoded.head(20))\n",
    "\n",
    "    # Print the last 20 rows\n",
    "    print(\"\\nLast 20 rows:\")\n",
    "    print(gdf_encoded.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric variables to be dropped\n",
    "coorelated_variables_to_drop = [\n",
    "    'maksimal5d', 'maksimal14', 'doegn10mm', 'doegn20mm', 'time2aarsh', 'time5aarsh', \n",
    "    'time10aars', 'time20aars', 'time50aars', 'time100aar', 'doegn5aars', 'doegn10aar', \n",
    "    'doegn20aar', 'doegn50aar', 'doegn100aa', 'toerredage', 'toerreperi', 'potentielf', \n",
    "    'solindstra', 'dagligmint', 'lavestetem', 'gennemsn_1', 'gennemsnit', 'varmeboelg', 'doegnetste', \n",
    "    'hedeboelge', 'hoejestete', 'vaekstsaes', 'ekstremvin', 'maksimaldo', 'skybrud', \n",
    "    'aaretstemp', 'e_value', 'g_value', 'count', 'building', 'clay_accu_', 'streamlake', 'sand_accu'\n",
    "]\n",
    "\n",
    "# Drop specified numeric variables and exclude non-numeric columns\n",
    "gdf_reduced = gdf_encoded.drop(columns=coorelated_variables_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d96f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns again\n",
    "with pd.option_context('display.max_columns', None, 'display.max_rows', None):\n",
    "    # Print the first 20 rows\n",
    "    print(\"First 20 rows:\")\n",
    "    print(gdf_reduced.head(20))\n",
    "\n",
    "    # Print the last 20 rows\n",
    "    print(\"\\nLast 20 rows:\")\n",
    "    print(gdf_reduced.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "nan_in_data = gdf_reduced.isnull().sum()\n",
    "\n",
    "# Print columns with NaN values and their count\n",
    "nan_columns = nan_in_data[nan_in_data > 0]\n",
    "print(\"Columns with NaN values and their count:\")\n",
    "print(nan_columns)\n",
    "\n",
    "# Check if there are any NaN values in the entire DataFrame\n",
    "if gdf_encoded.isnull().values.any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ba9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(gdf_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA and choose the number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the scaled data\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Explained variance ratio for each principal component\n",
    "print(\"Explained Variance Ratio for 2 components:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97451a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA and choose the number of components\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# Fit and transform the scaled data\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Explained variance ratio for each principal component\n",
    "print(\"Explained Variance Ratio for 10 components:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scree plot for all PCs\n",
    "\n",
    "# Determine the maximum number of components\n",
    "max_components = min(scaled_data.shape)  # Minimum of the number of samples and features\n",
    "\n",
    "# Perform PCA for the maximum possible components\n",
    "pca_max = PCA(n_components=max_components)\n",
    "pca_max_result = pca_max.fit_transform(scaled_data)\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_variance_max = pca_max.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Plot the scree plot for the maximum components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_components + 1), cumulative_variance_max, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot for Maximum Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the cumulative explained variance\n",
    "print(\"Cumulative Explained Variance for Maximum Components:\", cumulative_variance_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf31b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the loadings (coefficients of the PCA)\n",
    "coefficients = pca_2d.components_.T * np.sqrt(pca_2d.explained_variance_)\n",
    "\n",
    "# Ensure that the PCA results are scaled correctly\n",
    "pca_2d_result_scaled = pca_2d_result / pca_2d_result.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biplot for first two PCs \n",
    "def pca_biplot_with_variance(score, coeff, pc_var, labels=None):\n",
    "    xs = score[:, 0]\n",
    "    ys = score[:, 1]\n",
    "    n = coeff.shape[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Draw the vectors from the origin\n",
    "    for i in range(n):\n",
    "        ax.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5, head_width=0.05, head_length=0.1)\n",
    "        if labels is not None:\n",
    "            ax.text(coeff[i, 0], coeff[i, 1], labels[i], color='g', ha='center', va='center')\n",
    "\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.set_xlabel(f\"PC1 ({pc_var[0]:.2%} variance)\")\n",
    "    ax.set_ylabel(f\"PC2 ({pc_var[1]:.2%} variance)\")\n",
    "    ax.grid()\n",
    "\n",
    "# Explained variance for each principal component\n",
    "pc_variance = pca_2d.explained_variance_ratio_\n",
    "\n",
    "# Create the biplot with variance labels\n",
    "pca_biplot_with_variance(pca_2d_result_scaled, coefficients, pc_variance, labels=gdf_reduced.columns.values)\n",
    "plt.title('PCA Biplot with Variance for the First Two Principal Components')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second biplot (PC3 and PC4) all biplots were explored\n",
    "pca_4d = PCA(n_components=4)\n",
    "pca_4d_result = pca_4d.fit_transform(scaled_data_modified)\n",
    "\n",
    "# Extract the loadings for PC3 and PC4\n",
    "coefficients_3_4 = pca_4d.components_[2:4].T * np.sqrt(pca_4d.explained_variance_[2:4])\n",
    "\n",
    "def pca_biplot_3_4(score, coeff, pc_var, labels=None):\n",
    "    xs = score[:, 2]  # PC3\n",
    "    ys = score[:, 3]  # PC4\n",
    "    n = coeff.shape[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Draw the vectors from the origin\n",
    "    for i in range(n):\n",
    "        ax.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5, head_width=0.05, head_length=0.1)\n",
    "        if labels is not None:\n",
    "            ax.text(coeff[i, 0], coeff[i, 1], labels[i], color='g', ha='center', va='center')\n",
    "\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.set_xlabel(f\"PC3 ({pc_var[2]:.2%} variance)\")\n",
    "    ax.set_ylabel(f\"PC4 ({pc_var[3]:.2%} variance)\")\n",
    "    ax.grid()\n",
    "\n",
    "# Create the biplot for PC3 and PC4\n",
    "pca_biplot_3_4(pca_4d_result, coefficients_3_4, pca_4d.explained_variance_ratio_, labels=gdf_reduced_modified.columns.values)\n",
    "plt.title('PCA Biplot for PC3 and PC4')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scree plots using gdf_reduced which is the encoded gdf after dropping variables from correlation \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Standardize the Data\n",
    "scaler = StandardScaler()\n",
    "gdf_scaled = scaler.fit_transform(gdf_reduced)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(gdf_scaled)\n",
    "\n",
    "# Number of components\n",
    "num_components = pca.n_components_\n",
    "\n",
    "# Convert to DataFrame for further use\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Plot the scree plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_components + 1), cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot (including one-hot encoded variables)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
