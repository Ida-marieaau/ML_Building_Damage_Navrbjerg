{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71590601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing of BBR and damage data\n",
    "\n",
    "# This Notebook is a collection of the scripts that extracts and manipulates the BBR data into two datasets: \n",
    "# one of all BBR building data points in Denmark and one of all damage building points in Denmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopandas.tools import sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script that extracts the relevant data from the BygningList by streaming and parsing the collected BBR JSON file\n",
    "\n",
    "#The output directory is called 'Processed_withID' and later 'Batched_withID_onlyonce' because an initial extraction\n",
    "#mistakenly had left out the 'id_lokalId', which was important to keep track of individual buildings\n",
    "\n",
    "# Data paths\n",
    "large_file_path = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\BBR TOTAL\\\\BBR_total_20230130123400.json\"\n",
    "output_directory = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Processed_withID\\\\\"\n",
    "\n",
    "def extract_attributes(item):\n",
    "    attributes = [\n",
    "        'forretningshændelse', 'byg021BygningensAnvendelse', 'byg026Opførelsesår',\n",
    "        'byg027OmTilbygningsår', 'byg032YdervæggensMateriale','byg033Tagdækningsmateriale',\n",
    "        'byg054AntalEtager','byg056Varmeinstallation','byg404Koordinat','byg406Koordinatsystem', 'id_lokalId'\n",
    "    ]\n",
    "    return {attr: item.get(attr, None) for attr in attributes}\n",
    "\n",
    "# Parameters for splitting data\n",
    "chunk_size = 100000  # Number of buildings per file\n",
    "current_chunk = 0\n",
    "current_data = []\n",
    "\n",
    "# Start streaming and parsing the JSON file\n",
    "with open(large_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    bygning_list = ijson.items(file, 'BygningList.item')\n",
    "    for index, item in enumerate(bygning_list):\n",
    "        current_data.append(extract_attributes(item))\n",
    "        \n",
    "        # Splitting and saving the data every chunk_size items\n",
    "        if (index + 1) % chunk_size == 0:\n",
    "            with open(output_directory + f\"data_chunk_{current_chunk}.json\", 'w', encoding=\"utf-8\") as output_file:\n",
    "                json.dump(current_data, output_file)\n",
    "            current_data = []  # Clear the current data\n",
    "            current_chunk += 1\n",
    "            \n",
    "    # Saving remaining data that didn't make up a full chunk\n",
    "    if current_data:\n",
    "        with open(output_directory + f\"data_chunk_{current_chunk}.json\", 'w', encoding=\"utf-8\") as output_file:\n",
    "            json.dump(current_data, output_file)\n",
    "\n",
    "print(f\"Data processed into {current_chunk + 1} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the BygningList chunks into batches of 25 chunks per batch\n",
    "\n",
    "# Directory where the chunks are located\n",
    "input_directory = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Processed_withID\\\\\"\n",
    "\n",
    "# Number of chunks in each batch\n",
    "chunks_per_batch = 25\n",
    "\n",
    "# Iterate through each chunk and append the data to a batch list\n",
    "for batch in range(0, 399, chunks_per_batch):\n",
    "    batch_data = []\n",
    "    for index in range(batch, min(batch + chunks_per_batch, 399)):\n",
    "        with open(input_directory + f\"data_chunk_{index}.json\", 'r', encoding=\"utf-8\") as input_file:\n",
    "            data = json.load(input_file)\n",
    "            batch_data.extend(data)\n",
    "    \n",
    "    # Save each batch to a separate JSON file\n",
    "    output_path = f\"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Batched_withID\\\\batch_{batch//chunks_per_batch}.json\"\n",
    "    with open(output_path, 'w', encoding=\"utf-8\") as output_file:\n",
    "        json.dump(batch_data, output_file)\n",
    "\n",
    "    print(f\"Saved batch {batch//chunks_per_batch} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the first batch to ensure that the data is as expected\n",
    "\n",
    "# Load JSON data\n",
    "with open('C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Batched_withID_onlyonce\\\\batch_0.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "bygning_df = pd.DataFrame(data)\n",
    "\n",
    "# Set display options to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the table\n",
    "print(bygning_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f635f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was the extraction of the BygningList and the same was done for the EtageList in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ae190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script that extracts the relevant data from the EtageList by streaming and parsing the collected BBR JSON file\n",
    "#The relevant data from this list is for the basements\n",
    "#The data is saved in many small chunks\n",
    "\n",
    "# Define your data path for the large file\n",
    "large_file_path = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\BBR TOTAL\\\\BBR_total_20230130123400.json\"\n",
    "output_directory = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\EtageList\\\\Chunks\\\\\"  # Store processed files here\n",
    "\n",
    "def extract_attributes(item):\n",
    "    attributes = ['eta006BygningensEtagebetegnelse', 'eta020SamletArealAfEtage', 'bygning']\n",
    "    return {attr: item.get(attr, None) for attr in attributes}\n",
    "\n",
    "# Parameters for splitting data\n",
    "chunk_size = 100000  # Number of buildings per file\n",
    "current_chunk = 0\n",
    "current_data = []\n",
    "\n",
    "# Start streaming and parsing the JSON file\n",
    "with open(large_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    bygning_list = ijson.items(file, 'EtageList.item')\n",
    "    for index, item in enumerate(bygning_list):\n",
    "        current_data.append(extract_attributes(item))\n",
    "        \n",
    "        # Split and save the data every chunk_size items\n",
    "        if (index + 1) % chunk_size == 0:\n",
    "            with open(output_directory + f\"data_chunk_{current_chunk}.json\", 'w', encoding=\"utf-8\") as output_file:\n",
    "                json.dump(current_data, output_file)\n",
    "            current_data = []  # Clear the current data\n",
    "            current_chunk += 1\n",
    "            \n",
    "    # Save any remaining data that didn't make up a full chunk\n",
    "    if current_data:\n",
    "        with open(output_directory + f\"data_chunk_{current_chunk}.json\", 'w', encoding=\"utf-8\") as output_file:\n",
    "            json.dump(current_data, output_file)\n",
    "            \n",
    "print(f\"Data processed into {current_chunk + 1} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfa9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the EtageList chunks into batches of 25 chunks per batch\n",
    "\n",
    "# Directory where the chunks are located\n",
    "input_directory = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\EtageList\\\\Chunks\\\\\"\n",
    "\n",
    "# Number of chunks in each batch\n",
    "chunks_per_batch = 25\n",
    "\n",
    "# Iterate through each chunk and append the data to a batch list\n",
    "for batch in range(0, 731, chunks_per_batch):\n",
    "    batch_data = []\n",
    "    for index in range(batch, min(batch + chunks_per_batch, 399)):\n",
    "        with open(input_directory + f\"data_chunk_{index}.json\", 'r', encoding=\"utf-8\") as input_file:\n",
    "            data = json.load(input_file)\n",
    "            batch_data.extend(data)\n",
    "    \n",
    "    # Save each batch to a separate JSON file\n",
    "    output_path = f\"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\EtageList_2\\\\Batches\\\\batch_{batch//chunks_per_batch}.json\"\n",
    "    with open(output_path, 'w', encoding=\"utf-8\") as output_file:\n",
    "        json.dump(batch_data, output_file)\n",
    "\n",
    "    print(f\"Saved batch {batch//chunks_per_batch} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first bacth is explored to make sure it contains the data\n",
    "\n",
    "# Load JSON data\n",
    "with open('C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\EtageList\\\\Batches\\\\batch_0.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "bygning_df = pd.DataFrame(data)\n",
    "\n",
    "# Set display options to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the table\n",
    "print(bygning_df)\n",
    "\n",
    "# Summary of the BBR data in first batch\n",
    "print(\"Data Description:\")\n",
    "print(bygning_df.describe())\n",
    "\n",
    "# Describe only object (or string) columns\n",
    "print(bygning_df.describe(include=['object']))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(bygning_df.isnull().sum())\n",
    "\n",
    "# Count of unique values in each column (for non-numeric columns)\n",
    "for column in bygning_df.columns:\n",
    "    if bygning_df[column].dtype == 'object':\n",
    "        print(f\"\\nUnique values in {column}: {bygning_df[column].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9082d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the first batch, there were 37259 buildings with basements (out of 250.000)... \n",
    "#It was decided to keep exploring it as a binary variable\n",
    "print(bygning_df['eta006BygningensEtagebetegnelse'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The extracted data from the two lists are merged. \n",
    "#The data from EtageList is filtered ('def filter_rows') to create a binary basement variable (if a building has a basement \n",
    "#or not) and to add the data for the basement area for those buildings that has a basement\n",
    "\n",
    "# Load the batched data from BygningList\n",
    "bygninglist_data_dir = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Batched_withID\"\n",
    "bygninglist_files = [f for f in os.listdir(bygninglist_data_dir) if f.endswith('.json')]\n",
    "bygninglist_data = []\n",
    "for file in bygninglist_files:\n",
    "    full_path = os.path.join(bygninglist_data_dir, file)\n",
    "    bygninglist_data.append(pd.read_json(full_path))\n",
    "\n",
    "all_bygninglist_data = pd.concat(bygninglist_data, ignore_index=True)\n",
    "\n",
    "# Load the batched data from EtageList\n",
    "etagelist_data_dir = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\EtageList\\\\Batches\"\n",
    "etagelist_files = [f for f in os.listdir(etagelist_data_dir) if f.endswith('.json')]\n",
    "etagelist_data = []\n",
    "for file in etagelist_files:\n",
    "    full_path = os.path.join(etagelist_data_dir, file)\n",
    "    etagelist_data.append(pd.read_json(full_path))\n",
    "\n",
    "all_etagelist_data = pd.concat(etagelist_data, ignore_index=True)\n",
    "\n",
    "# Filter rows\n",
    "def filter_rows(group):\n",
    "    kl_row = group[group['eta006BygningensEtagebetegnelse'] == 'kl']\n",
    "    if kl_row.empty:\n",
    "        group['eta006BygningensEtagebetegnelse'] = None\n",
    "        group['eta020SamletArealAfEtage'] = None\n",
    "        return group.iloc[0]\n",
    "    else:\n",
    "        return kl_row.iloc[0]\n",
    "\n",
    "filtered_etagelist_df = all_etagelist_data.groupby('bygning').apply(filter_rows).reset_index(drop=True)\n",
    "\n",
    "# Merging the datasets\n",
    "merged_df = pd.merge(all_bygninglist_data, filtered_etagelist_df, left_on='id_lokalId', right_on='bygning', how='left')\n",
    "\n",
    "# Saving the merged dataset\n",
    "output_path = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Final\\\\Denmark_BBR.json\"\n",
    "merged_df.to_json(output_path, orient='records', lines=True)\n",
    "print(f\"Saved merged data to {output_path}\")\n",
    "\n",
    "# Data Visualization and Description\n",
    "print(\"Data Description:\")\n",
    "print(merged_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ea150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Denmark_BBR.json file was explored, and some manipulation and data preparation was made\n",
    "#the coordinates (points) were extracted based on the attribute byg404Koordinat and the data can then be read as a geodataframe\n",
    "#with its x and y coloumns\n",
    "#The construction year (byg026Opførelsesår) was manipulated to handle unrealistic values. Many buildings that are used as sheds,\n",
    "#garages etc. had a construction year of 1000. The oldest building in Denmark is from 1425.\n",
    "#The floors was manipulated similarly. The tallest building in Denmark is 30 floors.\n",
    "\n",
    "# Load Denmark_final_BBR.json file\n",
    "denmark_bbr = gpd.read_file(\"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Final\\\\Denmark_BBR.json\")\n",
    "\n",
    "# Display the first few rows to ensure it's loaded correctly\n",
    "print(denmark_bbr.head())\n",
    "\n",
    "def extract_coordinates(point_str):\n",
    "    try:\n",
    "        # Extract the coordinates from the POINT string\n",
    "        x, y = point_str.replace(\"POINT(\", \"\").replace(\")\", \"\").split()\n",
    "        return float(x), float(y)\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Extract x and y coordinates\n",
    "denmark_bbr['x'], denmark_bbr['y'] = zip(*denmark_bbr['byg404Koordinat'].map(extract_coordinates))\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame using the x and y columns as geometry\n",
    "gdf = gpd.GeoDataFrame(denmark_bbr, geometry=gpd.points_from_xy(denmark_bbr['x'], denmark_bbr['y']))\n",
    "\n",
    "# Set the CRS to EPSG:25832\n",
    "gdf.crs = \"EPSG:25832\"\n",
    "\n",
    "# Display the first few rows to ensure the geometry has been set correctly\n",
    "print(gdf.head())\n",
    "\n",
    "# Handle unrealistic construction year values\n",
    "mask_invalid_years1 = (gdf['byg026Opførelsesår'] <= 1425) | (gdf['byg026Opførelsesår'] >= 2023)\n",
    "gdf.loc[mask_invalid_years1, 'byg026Opførelsesår'] = np.nan\n",
    "\n",
    "mask_invalid_years2 = (gdf['byg027OmTilbygningsår'] < 1425) | (gdf['byg027OmTilbygningsår'] >= 2023)\n",
    "gdf.loc[mask_invalid_years2, 'byg027OmTilbygningsår'] = np.nan\n",
    "\n",
    "# Handle unrealistic values for 'byg054AntalEtager'\n",
    "# A reasonable range is from 0 to 30 floors\n",
    "mask_invalid_floors = (gdf['byg054AntalEtager'] < 0) | (gdf['byg054AntalEtager'] >= 30)\n",
    "gdf.loc[mask_invalid_floors, 'byg054AntalEtager'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some further exploration was done for missing values and dublicate 'id_lokalId'\n",
    "#Many buildings were dublicate\n",
    "\n",
    "# Check the number of rows\n",
    "num_rows = len(gdf)\n",
    "print(f\"Number of rows in the dataset: {num_rows}\")\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(\"\\nFirst 10 rows:\\n\")\n",
    "print(gdf.head(10))\n",
    "\n",
    "# Display the last 10 rows\n",
    "print(\"\\nLast 10 rows:\\n\")\n",
    "print(gdf.tail(10))\n",
    "\n",
    "missing = gdf.isnull().sum()\n",
    "percentage_missing = (gdf.isnull().sum()/len(gdf))*100\n",
    "missing_data = pd.DataFrame({'Number of Missing Values': missing, 'Percentage (%)': percentage_missing})\n",
    "print(\"\\nMissing Values:\\n\")\n",
    "print(missing_data)\n",
    "\n",
    "# Check dublicate values for id_lokalId\n",
    "unique_ids = gdf['id_lokalId'].nunique()\n",
    "if unique_ids == num_rows:\n",
    "    print(\"\\nEvery 'id_lokalId' is unique.\")\n",
    "else:\n",
    "    print(f\"\\nThere are {num_rows - unique_ids} 'id_lokalId' values that are not unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every case of dublicate id_lokalId was identical, and the GeoDataFrame was therefore filtered to keep only the first \n",
    "#occurrence of each id_lokalId. The new GeoDataFrame was saved. \n",
    "\n",
    "unique_gdf = gdf.drop_duplicates(subset='id_lokalId', keep='first')\n",
    "\n",
    "# Verify the number of unique rows\n",
    "print(f\"Number of unique rows based on 'id_lokalId': {len(unique_gdf)}\")\n",
    "\n",
    "# Save the filtered dataset to a new file\n",
    "output_path_unique = \"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Final\\\\Denmark_BBR_unique.geojson\"\n",
    "unique_gdf.to_file(output_path_unique, driver=\"GeoJSON\")\n",
    "print(f\"Saved unique data to {output_path_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89105f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script joins the damage case data to the nearest merged building point. \n",
    "\n",
    "#The damage case data from Geo's archive is not necessarily located precisely within the building polygons, but when exploring\n",
    "#the damage points, it often makes sense which building has endured damage, since the point is often right outside a \n",
    "#building polygon. \n",
    "#However, in many cases, the building that is nearest to the damage point is a shed, garage etc. And not likely to be \n",
    "#the actual damaged building of interest. Because of this, when joining the damage data to the BBR building point, \n",
    "#a buffer of 50m was created to select the nearest buildings of each damage point. The buildings that was then considered \n",
    "#unlikely to have endured damage was then excluded in the sense that, if the nearest building within the 50m buffer was a \n",
    "#garage, carport, shed, greenhouse, #canopy or conservatory, the next building was chosen (untill it was not one of these \n",
    "#building-uses).\n",
    "\n",
    "# Reading the manipulated BBR Denmark\n",
    "gdf = gpd.read_file(\"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Final\\\\Denmark_BBR_unique.geojson\")\n",
    "\n",
    "# Reading the damage case data\n",
    "damage_cases = gpd.read_file(\"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\Skadesager\\\\Damage_DK.shp\")\n",
    "damage_cases = damage_cases.to_crs('EPSG:25832')\n",
    "damage_cases['buffer'] = damage_cases.buffer(50)\n",
    "\n",
    "# Using a spatial index\n",
    "sindex = gdf.sindex\n",
    "\n",
    "# Placeholder\n",
    "closest_buildings = []\n",
    "\n",
    "# List of building uses to exclude ('910': 'Garage', '920': 'Carport', '930': 'Shed', '940': 'Greenhouse',\n",
    "#'950': 'Detached canopy', '960': 'Detached conservatory')\n",
    "excluded_uses = ['910', '920', '930', '940', '950', '960']\n",
    "\n",
    "#Loop that iterates over every damage point and finds the nearest building that meets the criteria\n",
    "for index, row in damage_cases.iterrows():\n",
    "    # Get the bounding box coordinates of the buffer for the current damage case\n",
    "    bounds = row['buffer'].bounds\n",
    "\n",
    "    # Use spatial index to find which buildings are within the bounding box of the buffer\n",
    "    possible_matches_index = list(sindex.intersection(bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "\n",
    "    # Check which of these possible matches are actually within the buffer\n",
    "    true_matches = possible_matches[possible_matches.intersects(row['buffer'])].copy()\n",
    "\n",
    "    # If there are matches within the buffer, calculate the distance\n",
    "    if not true_matches.empty:\n",
    "        true_matches['dist'] = true_matches.distance(row['geometry'])\n",
    "        \n",
    "        # Sort the buildings by distance\n",
    "        true_matches = true_matches.sort_values(by='dist')\n",
    "        \n",
    "        # Iterate through sorted buildings and select the first one that doesn't have an excluded use\n",
    "        selected_building = None\n",
    "        for _, building in true_matches.iterrows():\n",
    "            if building['byg021BygningensAnvendelse'] not in excluded_uses:\n",
    "                selected_building = building\n",
    "                break\n",
    "        \n",
    "        # Append the selected building, if any, to the results\n",
    "        if selected_building is not None:\n",
    "            closest_buildings.append(selected_building)\n",
    "\n",
    "# Convert results list to GeoDataFrame\n",
    "closest_buildings_gdf = gpd.GeoDataFrame(closest_buildings, columns=gdf.columns, crs=\"EPSG:25832\")\n",
    "\n",
    "# Handle byte data if necessary\n",
    "byte_cols = [col for col in closest_buildings_gdf.columns if closest_buildings_gdf[col].apply(lambda x: isinstance(x, bytes)).any()]\n",
    "for col in byte_cols:\n",
    "    closest_buildings_gdf[col] = closest_buildings_gdf[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "# Save to file\n",
    "closest_buildings_gdf.to_file(\"C:\\\\Users\\\\IM\\\\Desktop\\\\THESIS\\\\Data\\\\BBR\\\\Final\\\\Damage_final_BBR.geojson\", driver='GeoJSON')\n",
    "\n",
    "# Display results\n",
    "print(closest_buildings_gdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
